---
# Source: srv2/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: srv2
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: srv2/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: srv2-secret
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  username: YWRtaW4K
  password: cGFzc2FkbWluCg==
  POSTGRES_USER: YWRtaW4K
  POSTGRES_PASSWORD: cGFzc2FkbWluCg==
  spring.datasource.username: YWRtaW4K
  spring.datasource.password: cGFzc2FkbWluCg==
---
# Source: srv2/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: srv2-config
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
data:
  POSTGRES_DB: hw4_db
  POSTGRES_USER: admin
  POSTGRES_PASSWORD: passadmin
  PGDATA: /var/lib/postgresql/data/pgdata

  GF_SECURITY_ADMIN_USER: admin
  GF_SECURITY_ADMIN_PASSWORD: admin

  spring.datasource.url: jdbc:postgresql://localhost:5432/postgres
  spring.datasource.username: admin
  spring.datasource.password: passadmin
  jakarta.persistence.jdbc.url: jdbc:postgresql://localhost:5432/public/

  # This will drop any table in the database and create new ones base on the models
  spring.jpa.hibernate.ddl-auto: create-drop
  # update
  # none
#  global.scrape_interval: "5s"
    # By default, scrape targets every 15 seconds.

    # Attach these labels to any time series or alerts when communicating with
    # external systems (federation, remote storage, Alertmanager).
#  external_labels.monitor: 'codelab-monitor'

  # A scrape configuration containing exactly one endpoint to scrape:
  # Here it's Prometheus itself.
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  prometheus.yml: |
    global:
      scrape_interval: "5s" # By default, scrape targets every 15 seconds.
      # Attach these labels to any time series or alerts when communicating with
      # external systems (federation, remote storage, Alertmanager).
      external_labels:
        monitor: 'codelab-monitor'
    
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
    scrape_configs:
      - job_name: 'srv2'
        scrape_interval: 5s
        metrics_path: '/actuator/prometheus'
        static_configs:
          - targets: [ 'localhost:8000']
    
      - job_name: 'srv2-ingress'
        honor_timestamps: true
        scrape_interval: 5s
        scrape_timeout: 1m
        metrics_path: '/actuator/prometheus' #'/metrics'
        scheme: http
        static_configs:
          - targets: [srv2-ingress.default.svc.cluster.local:8000]
    

  alertmanager.yml: |
    
    global:
      # The smarthost and SMTP sender used for mail notifications.
      smtp_smarthost: 'localhost:25'
      smtp_from: 'alertmanager@example.org'

    # The root route on which each incoming alert enters.
    route:
      
      receiver: 'team-X-mails'

      group_by: ['alertname', 'cluster']
      group_wait: 30s
      group_interval: 5m

      repeat_interval: 3h

      # All the above attributes are inherited by all child routes and can
      # overwritten on each.

      # The child route trees.
      routes:
      # This route performs a regular expression match on alert labels to
      # catch alerts that are related to a list of services.
      - matchers:
        - service=~"^(srv2)$"
        receiver: team-X-mails


        routes:
        - matchers:
          - severity="critical"
          receiver: team-X-pager

      - matchers:
        - service="files"
        receiver: team-Y-mails

        routes:
        - matchers:
          - severity="critical"
          receiver: team-Y-pager

      # This route handles all alerts coming from a database service. If there's
      # no team to handle it, it defaults to the DB team.
      - matchers:
        - service="database"

        receiver: team-DB-pager
        # Also group alerts by affected database.
        group_by: [alertname, cluster, database]

        routes:
        - matchers:
          - owner="team-X"
          receiver: team-X-pager

        - matchers:
          - owner="team-Y"
          receiver: team-Y-pager

    inhibit_rules:
    - source_matchers:
        - severity="critical"
      target_matchers:
        - severity="warning"
      equal: ['alertname']

    receivers:
    - name: 'team-X-mails'
      email_configs:
      - to: 'team-X+alerts@example.org, team-Y+alerts@example.org'

    - name: 'team-X-pager'
      email_configs:
      - to: 'team-X+alerts-critical@example.org'
      pagerduty_configs:
      - routing_key: <team-X-key>

    - name: 'team-Y-mails'
      email_configs:
      - to: 'team-Y+alerts@example.org'

    - name: 'team-Y-pager'
      pagerduty_configs:
      - routing_key: <team-Y-key>

    - name: 'team-DB-pager'
      pagerduty_configs:
      - routing_key: <team-DB-key>
---
# Source: srv2/templates/service-ingress.yaml
apiVersion: v1
kind: Service
metadata:
  name: srv2-ingress
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 9901
      protocol: TCP
      name: metrics
  selector:
    app: controller-nginx-ingress
#    - port: 8000
#      targetPort: http
#      protocol: TCP
#      name: ingress-prometheus
---
# Source: srv2/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: srv2
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http
      protocol: TCP
      name: http
    - port: 9093
      targetPort: 9000
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
---
# Source: srv2/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: srv2
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: srv2
      app.kubernetes.io/instance: srv2
  template:
    metadata:
      labels:
        helm.sh/chart: srv2-0.1.0
        app.kubernetes.io/name: srv2
        app.kubernetes.io/instance: srv2
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: srv2
      securityContext:
        {}
      containers:
        - name: srv2
          securityContext:
            {}
          image: "slezkin71/microservice:srv3_3"
          envFrom:
            - configMapRef:
                name: srv2-config
          env:
              - name: spring.datasource.username1
                valueFrom:
                  secretKeyRef:
                    name: srv2-secret
                    key: username
              - name: spring.datasource.password1
                valueFrom:
                  secretKeyRef:
                    name: srv2-secret
                    key: password
          imagePullPolicy: Always
          args:
            - "--config.file=/config/prometheus.yml"
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/
              port: http
          readinessProbe:
            httpGet:
              path: /health/
              port: http
          resources:
            {}
        - name: srv2-db
          image: "postgres"
          envFrom:
            - configMapRef:
                name: srv2-config
          env:
              - name: POSTGRES_USER1
                valueFrom:
                  secretKeyRef:
                    name: srv2-secret
                    key: POSTGRES_USER
              - name: POSTGRES_PASSWORD1
                valueFrom:
                  secretKeyRef:
                    name: srv2-secret
                    key: POSTGRES_PASSWORD
          ports:
            - name: http
              containerPort: 5432
              protocol: TCP
        - name: prometheus
          image: prom/prometheus:latest
          envFrom:
            - configMapRef:
                name: srv2-config
          args:
            - --log.level=warn
            - --config.file=/etc/prometheus/prometheus.yml
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
              name: default
          volumeMounts:
            - name: config-volume
              mountPath: /etc/prometheus
        - name: alertmanager
          image: prom/alertmanager:latest
          envFrom:
            - configMapRef:
                name: srv2-config
          args:
            - --log.level=warn
            - --config.file=/etc/alertmanager/alertmanager.yml
          ports:
            - containerPort: 9093
              name: default
          volumeMounts:
            - name: config-volume
              mountPath: /etc/alertmanager
        - name: grafana
          image: grafana/grafana:main
          envFrom:
            - configMapRef:
                name: srv2-config
          ports:
            - containerPort: 3000
              name: default
      volumes:
        - configMap:
            name: srv2-config
          name: config-volume
#        - name: adminer
#          image: "adminer"
#          ports:
#          - name: http
#            containerPort: 8080
#            protocol: TCP
#        - name: prometheus
#          image: prom/prometheus
#          envFrom:
#            - configMapRef:
#                name: srv2-config
#          ports:
#            - name: http
#              port: 9090
#              containerPort: 9090
#              protocol: TCP
#            - 9090:9090
#          volumes:
#            - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
---
# Source: srv2/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: srv2-ingress
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  ingressClassName: nginx
  rules:
    - host: "arch.homework"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: srv2
                port:
                  number: 8000
---
# Source: srv2/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: srv2-monitor
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
#  namespace: monitoring
spec:
  endpoints:
    - port: "9093"
      scheme: http
      path: /metrics
      interval: 60s
  selector:
    matchLabels:
      app.kubernetes.io/name: srv2
      app.kubernetes.io/instance: srv2
  namespaceSelector:
    matchNames:
      - default
---
# Source: srv2/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "srv2-test-connection"
  labels:
    helm.sh/chart: srv2-0.1.0
    app.kubernetes.io/name: srv2
    app.kubernetes.io/instance: srv2
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['srv2:8000']
  restartPolicy: Never
